{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from combat.pycombat import pycombat\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../Dataset/Merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('MergedDataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 batches.\n",
      "Adjusting for 0 covariate(s) or covariate level(s).\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors.\n",
      "Finding parametric adjustments.\n",
      "Adjusting the Data\n"
     ]
    }
   ],
   "source": [
    "sampleID = dataset['SampleID']\n",
    "datasetID = dataset['SampleID'].apply(lambda x: x.split('-')[0]).values\n",
    "indicator = dataset['Label']\n",
    "dataset = dataset.drop(columns=['SampleID', 'Label'])\n",
    "\n",
    "dataset = pycombat(dataset.transpose(), datasetID).transpose()\n",
    "dataset.insert(0, 'SampleID', sampleID)\n",
    "dataset.insert(1, 'Label', indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         SampleID      PatientID  Label  \\\n",
      "0                        0-GSM1026056_600009.0001  0-600009.0001      1   \n",
      "1             0-GSM1026057_600009.0001-FollowUp_1  0-600009.0001      1   \n",
      "2                         0-GSM1026058_41461.0001   0-41461.0001      1   \n",
      "3                         0-GSM1026059_41462.0001   0-41462.0001      1   \n",
      "4                        0-GSM1026060_600029.0001  0-600029.0001      1   \n",
      "...                                           ...            ...    ...   \n",
      "2108  6-GSM2347715_NT142_W18D2-Control-FollowUp_1  6-NT142_W18D2      0   \n",
      "2109  6-GSM2347717_NT041_W18D2-Control-FollowUp_2  6-NT041_W18D2      0   \n",
      "2110  6-GSM2347719_NT142_W18D2-Control-FollowUp_2  6-NT142_W18D2      0   \n",
      "2111  6-GSM2347721_NT041_W18D2-Control-FollowUp_3  6-NT041_W18D2      0   \n",
      "2112  6-GSM2347723_NT142_W18D2-Control-FollowUp_3  6-NT142_W18D2      0   \n",
      "\n",
      "       SEC14L1     YIPF5    SLC1A5        C2      NOL6      TPM3    PSMD11  \\\n",
      "0     8.196784  7.331882  7.490453  5.134335  7.086093  6.575738  8.394630   \n",
      "1     8.527628  7.344338  7.138261  5.179817  7.057190  6.510802  8.436275   \n",
      "2     8.861115  7.385836  7.151080  5.327242  7.054991  7.116553  8.261701   \n",
      "3     8.369868  7.670419  7.065264  4.889047  7.046944  6.677731  8.372858   \n",
      "4     8.673693  7.488290  7.195899  5.399563  7.019457  6.447549  8.497332   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2108  8.065088  7.492929  7.253305  4.953809  6.875882  6.897800  8.369420   \n",
      "2109  9.252726  7.403881  7.339120  5.341230  6.822715  7.212214  8.349980   \n",
      "2110  7.391173  7.499315  7.138937  4.837130  6.805491  6.805091  8.357332   \n",
      "2111  9.246215  7.409623  7.443051  5.488207  6.889405  7.179457  8.377533   \n",
      "2112  7.627567  7.483506  7.174680  4.749858  6.797381  6.893837  8.368402   \n",
      "\n",
      "      ...    NDUFS3     GDPD5      RERE     ACAA1    CYP1B1     PALLD  \\\n",
      "0     ...  9.039925  7.850162  6.781086  8.785124  6.387533  5.662990   \n",
      "1     ...  9.109241  7.896896  6.809093  8.803807  6.447924  5.637888   \n",
      "2     ...  9.033793  7.912770  6.820119  8.840256  6.884564  5.671101   \n",
      "3     ...  9.078829  7.827901  6.808308  8.824410  7.062263  5.611400   \n",
      "4     ...  9.087302  7.841243  6.799674  8.793946  6.621094  5.576058   \n",
      "...   ...       ...       ...       ...       ...       ...       ...   \n",
      "2108  ...  9.101760  7.856311  6.771679  8.773067  6.790017  5.869378   \n",
      "2109  ...  9.066952  7.737311  6.786199  8.806082  6.937670  5.716358   \n",
      "2110  ...  9.067526  7.859737  6.774061  8.737787  6.776967  5.629630   \n",
      "2111  ...  9.125762  7.812505  6.789007  8.879557  6.929199  5.768002   \n",
      "2112  ...  9.077793  7.798221  6.798863  8.720931  6.801216  5.762732   \n",
      "\n",
      "         KRCC1     KCNG1    SREBF1      OPA1  \n",
      "0     7.515696  5.562299  7.190135  8.230196  \n",
      "1     7.523443  5.626888  7.343206  8.141547  \n",
      "2     7.568676  5.690521  7.663655  8.037954  \n",
      "3     7.820959  5.381907  6.947727  8.190153  \n",
      "4     7.665478  5.300326  7.054248  8.205019  \n",
      "...        ...       ...       ...       ...  \n",
      "2108  8.080074  6.041967  7.137242  8.242810  \n",
      "2109  7.877598  5.171707  7.129561  8.228746  \n",
      "2110  7.925033  5.204380  7.024428  8.232717  \n",
      "2111  7.877197  5.156811  7.281194  8.214981  \n",
      "2112  7.750139  5.422095  7.201624  8.180845  \n",
      "\n",
      "[2113 rows x 5588 columns]\n"
     ]
    }
   ],
   "source": [
    "def getPatientID(sampleID):\n",
    "    return sampleID.split('-')[0] + '-' + sampleID.split('-')[1].split('_', 1)[1]\n",
    "\n",
    "dataset.insert(1, 'PatientID', dataset['SampleID'].apply(getPatientID))\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "gruppi = dataset.groupby('PatientID')\n",
    "\n",
    "def sanity_check(gruppi):\n",
    "    for group_name, group_data in gruppi:\n",
    "        if 'Control' in group_data['SampleID'].iloc[0]:\n",
    "            for e in group_data['SampleID']:\n",
    "                if not 'Control' in e:\n",
    "                    print(\"Errore in gruppo:\", group_name)\n",
    "                    break\n",
    "        else:\n",
    "            for e in group_data['SampleID']:\n",
    "                if 'Control' in e:\n",
    "                    print(\"Errore in gruppo:\", group_name)\n",
    "                    break\n",
    "\n",
    "sanity_check(gruppi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset di train:\n",
      "(1593, 5588)\n",
      "I malati sono:  695\n",
      "I sani sono:  898\n",
      "\n",
      "Dataset di test:\n",
      "(520, 5588)\n",
      "I malati sono:  245\n",
      "I sani sono:  275\n"
     ]
    }
   ],
   "source": [
    "#n_splits number of re-shuffling & splitting iterations.\n",
    "#test_size represents the proportion of the dataset to include in the test split.\n",
    "#random_state is the seed used by the random number generator.\n",
    "splitter = GroupShuffleSplit(n_splits=2, test_size=0.25, random_state = 42)\n",
    "split = splitter.split(dataset, groups=dataset['PatientID'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train = dataset.iloc[train_inds].sample(frac=1, random_state=42)\n",
    "test = dataset.iloc[test_inds].sample(frac=1, random_state=42)\n",
    "\n",
    "print(\"Dataset di train:\")\n",
    "print(train.shape)\n",
    "print(\"I malati sono: \", sum(train['Label'] == 1))\n",
    "print(\"I sani sono: \", sum(train['Label'] == 0))\n",
    "\n",
    "print(\"\\nDataset di test:\")\n",
    "print(test.shape)\n",
    "print(\"I malati sono: \", sum(test['Label'] == 1))\n",
    "print(\"I sani sono: \", sum(test['Label'] == 0))\n",
    "\n",
    "#malati train 45.12 perc\n",
    "#malati test 42.49 perc\n",
    "#malati sul totale 44.49 perc\n",
    "\n",
    "y_train = train['Label']\n",
    "x_train = train.drop(columns=['SampleID', 'Label', 'PatientID'])\n",
    "\n",
    "y_test = test['Label']\n",
    "x_test = test.drop(columns=['SampleID', 'Label', 'PatientID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, hyperparameters, x_train, y_train, printTrain, njobs): \n",
    "    pipeline = Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', model)])\n",
    "    gridSearch = GridSearchCV(pipeline, param_grid=hyperparameters, cv=5, return_train_score=True, refit=True, n_jobs=njobs, verbose=10, error_score='raise') if printTrain == True else GridSearchCV(pipeline, param_grid=hyperparameters, cv=5, return_train_score=False, refit=True, n_jobs=njobs, verbose=10, error_score='raise')\n",
    "    gridSearch.fit(x_train, y_train)\n",
    "    print(\"Best model:\", gridSearch.best_estimator_, gridSearch.best_params_)\n",
    "    print(\"Best score:\", np.max(gridSearch.cv_results_['mean_test_score']))\n",
    "    print(\"All scores:\", gridSearch.cv_results_['mean_test_score'])\n",
    "    return gridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = trainModel(SVC(), {\n",
    "    'classifier__kernel': ['linear', 'poly', 'rbf'], \n",
    "    'classifier__probability': [True],\n",
    "    'classifier__C': [2**-5, 2**-3, 2**-1, 2**0, 2**1, 2**3, 2**7, 2**9, 2**11, 2**13, 2**15],\n",
    "    'classifier__gamma': [2**-15, 2**-13, 2**-11, 2**-9, 2**-7, 2**-5, 2**-3, 2**-1, 2**1, 2**3, 2**5, 'scale', 'auto']}, x_train, y_train, False, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier', SVC(C=128, gamma=0.0078125))]) {'classifier__C': 128, 'classifier__gamma': 0.0078125, 'classifier__kernel': 'rbf'}\n",
      "Best score: 0.9447526665483725\n",
      "All scores: [0.5813036  0.56371523 0.56371523 0.5813036  0.56371523 0.56371523\n",
      " 0.5813036  0.56371523 0.56371523 0.5813036  0.56371523 0.56371523\n",
      " 0.5813036  0.85562587 0.56371523 0.5813036  0.85939552 0.56371523\n",
      " 0.5813036  0.85939552 0.56371523 0.5813036  0.85939552 0.56371523\n",
      " 0.5813036  0.85939552 0.56371523 0.5813036  0.85939552 0.56371523\n",
      " 0.5813036  0.85939552 0.56371523 0.5813036  0.86002248 0.56371523\n",
      " 0.5813036  0.56371523 0.56371523 0.56180675 0.56371523 0.56371523\n",
      " 0.56180675 0.56371523 0.56371523 0.56180675 0.56371523 0.56371523\n",
      " 0.56180675 0.57753002 0.56371523 0.56180675 0.85939552 0.56371523\n",
      " 0.56180675 0.85939552 0.78465724 0.56180675 0.85939552 0.56371523\n",
      " 0.56180675 0.85939552 0.56371523 0.56180675 0.85939552 0.56371523\n",
      " 0.56180675 0.85939552 0.56371523 0.56180675 0.85939552 0.56371523\n",
      " 0.56180675 0.85939552 0.57438339 0.56180675 0.56371523 0.56371523\n",
      " 0.5605548  0.56371523 0.56371523 0.5605548  0.56371523 0.56371523\n",
      " 0.5605548  0.56371523 0.56371523 0.5605548  0.7721358  0.56371523\n",
      " 0.5605548  0.85939552 0.86002445 0.5605548  0.85939552 0.89202303\n",
      " 0.5605548  0.85939552 0.61265748 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.87321425\n",
      " 0.5605548  0.56371523 0.56371523 0.5605548  0.56371523 0.56371523\n",
      " 0.5605548  0.56371523 0.56371523 0.5605548  0.56371523 0.56371523\n",
      " 0.5605548  0.83555332 0.58693441 0.5605548  0.85939552 0.90271485\n",
      " 0.5605548  0.85939552 0.92278741 0.5605548  0.85939552 0.6973995\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.90899036 0.5605548  0.56371523 0.56371523\n",
      " 0.5605548  0.56371523 0.56371523 0.5605548  0.56371523 0.56371523\n",
      " 0.5605548  0.56371523 0.56371523 0.5605548  0.85562587 0.7972398\n",
      " 0.5605548  0.85939552 0.92906883 0.5605548  0.85939552 0.93471146\n",
      " 0.5605548  0.85939552 0.72062459 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.93283453\n",
      " 0.5605548  0.56371523 0.56371523 0.5605548  0.56371523 0.56371523\n",
      " 0.5605548  0.56371523 0.56371523 0.5605548  0.57753002 0.64531456\n",
      " 0.5605548  0.85939552 0.89706828 0.5605548  0.85939552 0.94286785\n",
      " 0.5605548  0.85939552 0.93408253 0.5605548  0.85939552 0.72062459\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.94035804 0.5605548  0.56371523 0.56371523\n",
      " 0.5605548  0.56371523 0.57815895 0.5605548  0.56371523 0.66981526\n",
      " 0.5605548  0.85562587 0.86190138 0.5605548  0.85939552 0.92404132\n",
      " 0.5605548  0.85939552 0.94475267 0.5605548  0.85939552 0.93408253\n",
      " 0.5605548  0.85939552 0.72062459 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.94035804\n",
      " 0.5605548  0.56999862 0.73634195 0.5605548  0.56371523 0.60766941\n",
      " 0.5605548  0.57753002 0.74198064 0.5605548  0.85939552 0.86126062\n",
      " 0.5605548  0.85939552 0.92404132 0.5605548  0.85939552 0.94475267\n",
      " 0.5605548  0.85939552 0.93408253 0.5605548  0.85939552 0.72062459\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.94035804 0.5605548  0.73258217 0.78404606\n",
      " 0.5605548  0.56371523 0.631484   0.5605548  0.7721358  0.74072475\n",
      " 0.5605548  0.85939552 0.86126062 0.5605548  0.85939552 0.92404132\n",
      " 0.5605548  0.85939552 0.94475267 0.5605548  0.85939552 0.93408253\n",
      " 0.5605548  0.85939552 0.72062459 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.94035804\n",
      " 0.5605548  0.84935234 0.78467696 0.5605548  0.56371523 0.63525759\n",
      " 0.5605548  0.85562587 0.74072475 0.5605548  0.85939552 0.86126062\n",
      " 0.5605548  0.85939552 0.92404132 0.5605548  0.85939552 0.94475267\n",
      " 0.5605548  0.85939552 0.93408253 0.5605548  0.85939552 0.72062459\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.94035804 0.5605548  0.85939552 0.78467696\n",
      " 0.5605548  0.57753002 0.63525759 0.5605548  0.85939552 0.74072475\n",
      " 0.5605548  0.85939552 0.86126062 0.5605548  0.85939552 0.92404132\n",
      " 0.5605548  0.85939552 0.94475267 0.5605548  0.85939552 0.93408253\n",
      " 0.5605548  0.85939552 0.72062459 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.56371523\n",
      " 0.5605548  0.85939552 0.56371523 0.5605548  0.85939552 0.94035804\n",
      " 0.5605548  0.85939552 0.78467696]\n"
     ]
    }
   ],
   "source": [
    "svc = joblib.load('../../Modelli/DatasetFull/svc.pkl')\n",
    "print(\"Best model:\", svc.best_estimator_, svc.best_params_)\n",
    "print(\"Best score:\", np.max(svc.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", svc.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = trainModel(RandomForestClassifier(), {\n",
    "    'classifier__n_estimators': [i for i in range(150, 251, 25)],\n",
    "    'classifier__max_depth': [i for i in range(4, 13)]}, x_train, y_train, True, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 RandomForestClassifier(max_depth=8, n_estimators=175))]) {'classifier__max_depth': 8, 'classifier__n_estimators': 175}\n",
      "Best score: 0.9384791309319611\n",
      "All scores: [0.91964669 0.9209085  0.91399815 0.92153546 0.92154334 0.92216833\n",
      " 0.92153152 0.92592812 0.92404921 0.92153546 0.92843595 0.92780702\n",
      " 0.92781491 0.92654522 0.92844384 0.92969579 0.92404921 0.93220757\n",
      " 0.92906883 0.92781688 0.93282664 0.93847913 0.92968987 0.93157666\n",
      " 0.93283847 0.93157864 0.93094773 0.92593009 0.9290708  0.9284399\n",
      " 0.93157469 0.9347154  0.93408253 0.93597129 0.92906883 0.92906488\n",
      " 0.93095365 0.92717809 0.93032275 0.93471146 0.92969382 0.93409042\n",
      " 0.93220757 0.93220362 0.93408253]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.95       275\n",
      "           1       0.91      0.98      0.94       245\n",
      "\n",
      "    accuracy                           0.94       520\n",
      "   macro avg       0.94      0.95      0.94       520\n",
      "weighted avg       0.95      0.94      0.94       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest = joblib.load('../../Modelli/DatasetFull/randomForest.pkl')\n",
    "print(\"Best model:\", randomForest.best_estimator_, randomForest.best_params_)\n",
    "print(\"Best score:\", np.max(randomForest.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", randomForest.cv_results_['mean_test_score'])\n",
    "\n",
    "y_pred = randomForest.predict(x_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticNet = trainModel(LogisticRegression(), {\n",
    "    'classifier__C': [2**-7, 2**-5, 2**-3, 2**-1, 2**0, 2**1, 2**3, 2**7, 2**9, 2**11],\n",
    "    'classifier__penalty': ['elasticnet'],\n",
    "    'classifier__l1_ratio': [0, 0.0001, 0.1, 0.25, 0.5, 0.75, 1],\n",
    "    'classifier__solver': ['saga']}, x_train, y_train, True, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.5, l1_ratio=1, penalty='elasticnet',\n",
      "                                    solver='saga'))]) {'classifier__C': 0.5, 'classifier__l1_ratio': 1, 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga'}\n",
      "Best score: 0.7061907296780425\n",
      "All scores: [0.55494568 0.5561996  0.56371523 0.56371523 0.56371523 0.56371523\n",
      " 0.56371523 0.54801759 0.54801759 0.57501429 0.5655961  0.56371523\n",
      " 0.56371523 0.56371523 0.56620532 0.56494943 0.62899194 0.68548136\n",
      " 0.64027326 0.56997496 0.55616609 0.5611995  0.56245736 0.58442263\n",
      " 0.62021056 0.6610122  0.68736224 0.70619073 0.56182843 0.56057452\n",
      " 0.57061178 0.59007512 0.62083555 0.64281067 0.66352004 0.55931863\n",
      " 0.56120147 0.56559413 0.57249463 0.58944816 0.60703259 0.62398218\n",
      " 0.56120147 0.55994756 0.56371128 0.56496914 0.56747895 0.57187161\n",
      " 0.57374855 0.56246131 0.55868969 0.55868969 0.56057649 0.56057254\n",
      " 0.56120739 0.56245736 0.56308432 0.56057452 0.55994361 0.55931863\n",
      " 0.55869167 0.55994558 0.56057649 0.55931863 0.56120147 0.56120147\n",
      " 0.56245736 0.56057649 0.55743775 0.56245934]\n"
     ]
    }
   ],
   "source": [
    "elasticNet = joblib.load('../../Modelli/DatasetFull/elasticNet.pkl')\n",
    "print(\"Best model:\", elasticNet.best_estimator_, elasticNet.best_params_)\n",
    "print(\"Best score:\", np.max(elasticNet.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", elasticNet.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = trainModel(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'classifier__weights': ['uniform', 'distance'] }, x_train, y_train, True, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier', KNeighborsClassifier(n_neighbors=3))]) {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}\n",
      "Best score: 0.8920328857869521\n",
      "All scores: [0.89203289 0.89203289 0.87570237 0.87570237 0.84555904 0.84555904\n",
      " 0.81730841 0.81730841 0.78278031 0.78340924 0.75578557 0.7601802\n",
      " 0.73319138 0.73883796]\n"
     ]
    }
   ],
   "source": [
    "knn = joblib.load('../../Modelli/DatasetFull/knn.pkl')\n",
    "print(\"Best model:\", knn.best_estimator_, knn.best_params_)\n",
    "print(\"Best score:\", np.max(knn.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", knn.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistGradientBoosting = trainModel(HistGradientBoostingClassifier(), {\n",
    "    'classifier__max_depth': [3, 5, 7, 9, 11],\n",
    "    'classifier__max_iter': [100, 150, 200, 250],\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001]}, x_train, y_train, False, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 HistGradientBoostingClassifier(max_depth=9, max_iter=200))]) {'classifier__learning_rate': 0.1, 'classifier__max_depth': 9, 'classifier__max_iter': 200}\n",
      "Best score: 0.9491433528518758\n",
      "All scores: [0.94099091 0.94789338 0.9453816  0.94476055 0.94664143 0.94726839\n",
      " 0.9453816  0.94537765 0.94789535 0.94537568 0.94600659 0.94914335\n",
      " 0.90333984 0.91338105 0.92154137 0.91211924 0.92279135 0.93157469\n",
      " 0.90396088 0.91964867 0.92968987 0.90458784 0.9202776  0.93094773\n",
      " 0.56371523 0.66482325 0.83742237 0.56371523 0.82926007 0.84871552\n",
      " 0.56371523 0.82110566 0.84997536 0.56371523 0.82110566 0.84997536]\n"
     ]
    }
   ],
   "source": [
    "HistGradientBoosting = joblib.load('../../Modelli/DatasetFull/HistGradientBoostingWithFold.pkl')\n",
    "print(\"Best model:\", HistGradientBoosting.best_estimator_, HistGradientBoosting.best_params_)\n",
    "print(\"Best score:\", np.max(HistGradientBoosting.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", HistGradientBoosting.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoosting = trainModel(GradientBoostingClassifier(), {\n",
    "    'classifier__max_depth': [3, 5, 7, 9, 11],\n",
    "    'classifier__max_iter': [100, 150, 200, 250],\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001]}, x_train, y_train, False, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 150, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "Best score:  1.0\n"
     ]
    }
   ],
   "source": [
    "GradientBoosting = joblib.load('../../Modelli/DatasetFull/GradientBoostingWithFold.pkl')\n",
    "print(\"Best params: \", GradientBoosting)\n",
    "print(\"Best score: \",GradientBoosting.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayes = Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', GaussianNB())])\n",
    "print(naiveBayes.fit(x_train, y_train).score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9152542372881356\n"
     ]
    }
   ],
   "source": [
    "naiveBayes = joblib.load('../../Modelli/DatasetFull/naiveBayes.pkl')\n",
    "print(naiveBayes.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance dei vari modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC:\n",
      "Iperparametri:  {'classifier__C': 128, 'classifier__gamma': 0.0078125, 'classifier__kernel': 'rbf'}\n",
      "Training accuracy:  0.9447526665483725\n",
      "Test accuracy:  0.95 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       275\n",
      "           1       0.94      0.96      0.95       245\n",
      "\n",
      "    accuracy                           0.95       520\n",
      "   macro avg       0.95      0.95      0.95       520\n",
      "weighted avg       0.95      0.95      0.95       520\n",
      " \n",
      "\n",
      "\n",
      "Random Forest:\n",
      "Iperparametri:  {'classifier__max_depth': 8, 'classifier__n_estimators': 175}\n",
      "Training accuracy:  0.9384791309319611\n",
      "Test accuracy:  0.9442307692307692 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.95       275\n",
      "           1       0.91      0.98      0.94       245\n",
      "\n",
      "    accuracy                           0.94       520\n",
      "   macro avg       0.94      0.95      0.94       520\n",
      "weighted avg       0.95      0.94      0.94       520\n",
      " \n",
      "\n",
      "\n",
      "Elastic Net:\n",
      "Iperparametri:  {'classifier__C': 0.5, 'classifier__l1_ratio': 1, 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga'}\n",
      "Training accuracy:  0.7061907296780425\n",
      "Test accuracy:  0.6865384615384615 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72       275\n",
      "           1       0.70      0.58      0.64       245\n",
      "\n",
      "    accuracy                           0.69       520\n",
      "   macro avg       0.69      0.68      0.68       520\n",
      "weighted avg       0.69      0.69      0.68       520\n",
      " \n",
      "\n",
      "\n",
      "KNN:\n",
      "Iperparametri:  {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}\n",
      "Training accuracy:  0.8920328857869521\n",
      "Test accuracy:  0.8826923076923077 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88       275\n",
      "           1       0.84      0.93      0.88       245\n",
      "\n",
      "    accuracy                           0.88       520\n",
      "   macro avg       0.88      0.89      0.88       520\n",
      "weighted avg       0.89      0.88      0.88       520\n",
      " \n",
      "\n",
      "\n",
      "Hist Gradient Boosting:\n",
      "Iperparametri:  {'classifier__learning_rate': 0.1, 'classifier__max_depth': 9, 'classifier__max_iter': 200}\n",
      "Training accuracy:  0.9491433528518758\n",
      "Test accuracy:  0.9538461538461539 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96       275\n",
      "           1       0.93      0.97      0.95       245\n",
      "\n",
      "    accuracy                           0.95       520\n",
      "   macro avg       0.95      0.95      0.95       520\n",
      "weighted avg       0.95      0.95      0.95       520\n",
      " \n",
      "\n",
      "\n",
      "Gradient Boosting:\n",
      "Iperparametri:  Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 GradientBoostingClassifier(max_depth=5, n_estimators=150))])\n",
      "Training accuracy:  1.0\n",
      "Test accuracy:  0.9442307692307692 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       275\n",
      "           1       0.92      0.96      0.94       245\n",
      "\n",
      "    accuracy                           0.94       520\n",
      "   macro avg       0.94      0.95      0.94       520\n",
      "weighted avg       0.95      0.94      0.94       520\n",
      " \n",
      "\n",
      "\n",
      "Naive Bayes:\n",
      "Iperparametri:  Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', GaussianNB())])\n",
      "Training accuracy:  0.9152542372881356\n",
      "Test accuracy:  0.9403846153846154 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       275\n",
      "           1       0.90      0.99      0.94       245\n",
      "\n",
      "    accuracy                           0.94       520\n",
      "   macro avg       0.94      0.94      0.94       520\n",
      "weighted avg       0.94      0.94      0.94       520\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prettyPrint(model, name):\n",
    "    if isinstance(model, Pipeline):\n",
    "        print(name + \":\")\n",
    "        print(\"Iperparametri: \", model)\n",
    "        print(\"Training accuracy: \", model.score(x_train, y_train))\n",
    "        print(\"Test accuracy: \", model.score(x_test, y_test), \"\\n\")\n",
    "        print(classification_report(y_test, model.predict(x_test)), \"\\n\\n\")\n",
    "    else:\n",
    "        print(name + \":\")\n",
    "        print(\"Iperparametri: \", model.best_params_)\n",
    "        print(\"Training accuracy: \", model.best_score_)\n",
    "        print(\"Test accuracy: \", model.score(x_test, y_test), \"\\n\")\n",
    "        print(classification_report(y_test, model.predict(x_test)), \"\\n\\n\")\n",
    "\n",
    "for model, name in [(svc, \"SVC\"), (randomForest, \"Random Forest\"), (elasticNet, \"Elastic Net\"), (knn, \"KNN\"), (HistGradientBoosting, \"Hist Gradient Boosting\"), (GradientBoosting, \"Gradient Boosting\"), (naiveBayes, \"Naive Bayes\")]:\n",
    "    prettyPrint(model, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
