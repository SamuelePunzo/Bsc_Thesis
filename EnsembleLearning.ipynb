{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from combat.pycombat import pycombat\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../Dataset/Merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione Training e Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('MergedDataset.csv', index_col=0)\n",
    "\n",
    "sampleID = dataset['SampleID']\n",
    "datasetID = dataset['SampleID'].apply(lambda x: x.split('-')[0]).values\n",
    "indicator = dataset['Label']\n",
    "dataset = dataset.drop(columns=['SampleID', 'Label'])\n",
    "\n",
    "dataset = pycombat(dataset.transpose(), datasetID).transpose()\n",
    "dataset.insert(0, 'SampleID', sampleID)\n",
    "dataset.insert(1, 'Label', indicator)\n",
    "\n",
    "def getPatientID(sampleID):\n",
    "    return sampleID.split('-')[0] + '-' + sampleID.split('-')[1].split('_', 1)[1]\n",
    "\n",
    "dataset.insert(1, 'PatientID', dataset['SampleID'].apply(getPatientID))\n",
    "print(dataset)\n",
    "\n",
    "gruppi = dataset.groupby('PatientID')\n",
    "\n",
    "def sanity_check(gruppi):\n",
    "    for group_name, group_data in gruppi:\n",
    "        if 'Control' in group_data['SampleID'].iloc[0]:\n",
    "            for e in group_data['SampleID']:\n",
    "                if not 'Control' in e:\n",
    "                    print(\"Errore in gruppo:\", group_name)\n",
    "                    break\n",
    "        else:\n",
    "            for e in group_data['SampleID']:\n",
    "                if 'Control' in e:\n",
    "                    print(\"Errore in gruppo:\", group_name)\n",
    "                    break\n",
    "\n",
    "sanity_check(gruppi)\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=2, test_size=0.25, random_state = 42)\n",
    "split = splitter.split(dataset, groups=dataset['PatientID'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train = dataset.iloc[train_inds].sample(frac=1, random_state=42)\n",
    "test = dataset.iloc[test_inds].sample(frac=1, random_state=42)\n",
    "\n",
    "print(\"Dataset di train:\")\n",
    "print(train.shape)\n",
    "print(\"I malati sono: \", sum(train['Label'] == 1))\n",
    "print(\"I sani sono: \", sum(train['Label'] == 0))\n",
    "\n",
    "print(\"\\nDataset di test:\")\n",
    "print(test.shape)\n",
    "print(\"I malati sono: \", sum(test['Label'] == 1))\n",
    "print(\"I sani sono: \", sum(test['Label'] == 0))\n",
    "\n",
    "y_train = train['Label']\n",
    "x_train = train.drop(columns=['SampleID', 'Label', 'PatientID'])\n",
    "\n",
    "y_test = test['Label']\n",
    "x_test = test.drop(columns=['SampleID', 'Label', 'PatientID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caricamento modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = joblib.load('../../Modelli/DatasetZeroes/svc.pkl')\n",
    "randomForest = joblib.load('../../Modelli/DatasetZeroes/randomForest.pkl')\n",
    "elasticNet = joblib.load('../../Modelli/DatasetZeroes/elasticNet.pkl')\n",
    "knn = joblib.load('../../Modelli/DatasetZeroes/knn.pkl')\n",
    "HistgradientBoosting = joblib.load('../../Modelli/DatasetZeroes/HistgradientBoostingWithFold.pkl')\n",
    "GradientBoosting = joblib.load('../../Modelli/DatasetZeroes/GradientBoostingWithFold.pkl')\n",
    "naiveBayes = joblib.load('../../Modelli/DatasetZeroes/naiveBayes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modello Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: \n",
    "<br>\n",
    "Nel caso datasetFull, la gridSearch con svc non ha probability = True\n",
    "<br>\n",
    "HistGradientBoosting si chiama histGradientBoostingWithFold\n",
    "<br>\n",
    "gradientBoosting non ha la gridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSvc = SVC(C=svc.best_params_['classifier__C'], kernel=svc.best_params_['classifier__kernel'], gamma=svc.best_params_['classifier__gamma'], probability=True)\n",
    "newRandomForest = RandomForestClassifier(n_estimators=randomForest.best_params_['classifier__n_estimators'], max_depth=randomForest.best_params_['classifier__max_depth'])\n",
    "newElasticNet = LogisticRegression(penalty=elasticNet.best_params_['classifier__penalty'], C=elasticNet.best_params_['classifier__C'], l1_ratio=elasticNet.best_params_['classifier__l1_ratio'], solver=elasticNet.best_params_['classifier__solver'])\n",
    "newKnn = KNeighborsClassifier(n_neighbors=knn.best_params_['classifier__n_neighbors'], weights=knn.best_params_['classifier__weights'])\n",
    "newHistGradientBoosting = HistGradientBoostingClassifier(learning_rate=HistgradientBoosting.best_params_['classifier__learning_rate'], max_iter=HistgradientBoosting.best_params_['classifier__max_iter'], max_depth=HistgradientBoosting.best_params_['classifier__max_depth'])\n",
    "newGradientBoosting = GradientBoostingClassifier(learning_rate=gradientBoosting.best_params_['classifier__learning_rate'], n_estimators=gradientBoosting.best_params_['classifier__n_estimators'], max_depth=gradientBoosting.best_params_['classifier__max_depth'])\n",
    "# newGradientBoosting = GradientBoostingClassifier(max_depth=5, n_estimators=150)\n",
    "newNaiveBayes = GaussianNB();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembleModel = VotingClassifier(estimators=[('svc', newSvc), ('randomForest', newRandomForest), ('HistGradientBoosting', newHistGradientBoosting), ('gradientBoosting', newGradientBoosting), ('naiveBayes', newNaiveBayes)], voting='hard')\n",
    "ensemblePipeline = Pipeline(steps=[('scaler', MinMaxScaler()), ('classifier', ensembleModel)])\n",
    "ensemblePipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensemble model:\")\n",
    "print(\"Iperparametri: \", ensemblePipeline.named_steps['classifier'])\n",
    "print(\"Training accuracy: \", ensemblePipeline.score(x_train, y_train))\n",
    "print(\"Test accuracy: \", ensemblePipeline.score(x_test, y_test))\n",
    "print(classification_report(y_test, ensemblePipeline.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightSum = sum([model.best_score_ for model in [svc, randomForest, HistgradientBoosting, gradientBoosting]])\n",
    "# weightSum += gradientBoosting.score(x_train, y_train)\n",
    "weightSum += naiveBayes.score(x_train, y_train)\n",
    "weights = [model.best_score_ / weightSum for model in [svc, randomForest, HistgradientBoosting, gradientBoosting]]\n",
    "# weights.append(gradientBoosting.score(x_train, y_train) / weightSum)\n",
    "weights.append(naiveBayes.score(x_train, y_train) / weightSum)\n",
    "\n",
    "ensembleModelWeighted = VotingClassifier(estimators=[('svc', newSvc), ('randomForest', newRandomForest), ('HistgradientBoosting', newHistGradientBoosting), ('gradientBoosting', newGradientBoosting), ('naiveBayes', newNaiveBayes)],voting='soft', weights=weights, n_jobs=-1)\n",
    "ensemblePipeline2 = Pipeline(steps=[('scaler', MinMaxScaler()), ('classifier', ensembleModelWeighted)])\n",
    "ensemblePipeline2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensemble weighted model:\")\n",
    "print(\"Iperparametri: \", ensemblePipeline2.named_steps['classifier'])\n",
    "print(\"Training accuracy: \", ensemblePipeline2.score(x_train, y_train))\n",
    "print(\"Test accuracy: \", ensemblePipeline2.score(x_test, y_test))\n",
    "print(classification_report(y_test, ensemblePipeline2.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyEnsemble(x, models):\n",
    "    Y_pred_proba_ensemble = [0 for i in range(len(x))]\n",
    "    count = 0\n",
    "    for i in range(len(models)):\n",
    "        gs=models[i]\n",
    "        count+=1\n",
    "        Y_pred_proba = gs.predict_proba(x)\n",
    "        Y_pred_proba_ensemble = [Y_pred_proba_ensemble[k] + Y_pred_proba[k, 1] for k in range(len(x))]\n",
    "    Y_pred_proba_ensemble = np.array(Y_pred_proba_ensemble)\n",
    "    Y_pred_proba_ensemble = Y_pred_proba_ensemble / count\n",
    "    Y_pred_ensemble = (Y_pred_proba_ensemble > 0.5)*1\n",
    "    return Y_pred_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [svc, randomForest, HistgradientBoosting, gradientBoosting, naiveBayes]\n",
    "trainResult = applyEnsemble(x_train, models)\n",
    "testResult = applyEnsemble(x_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Manual ensemble model:\")\n",
    "print(\"Models: svc, randomForest, HistgradientBoosting, gradientBoosting, naiveBayes\")\n",
    "print(\"Training accuracy: \", sum(trainResult == y_train) / len(y_train))\n",
    "print(\"Test accuracy: \", sum(testResult == y_test) / len(y_test))\n",
    "print(classification_report(y_test, testResult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DatasetFull </h1>\n",
    "\n",
    "| Modello                     | Iperparametri                                                                                                               | Training Accuracy | Test Accuracy | Precision | Recall | F1-score | Support |\n",
    "|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------|-------------------|---------------|-----------|--------|----------|---------|\n",
    "| SVC                         | {'classifier__C': 128, 'classifier__gamma': 0.0078125, 'classifier__kernel': 'rbf'}                                      | 0.9448            | 0.95          | 0.95      | 0.95   | 0.95     | 520     |\n",
    "| Random Forest               | {'classifier__max_depth': 9, 'classifier__n_estimators': 200}                                                             | 0.9347            | 0.9423        | 0.94      | 0.94   | 0.94     | 520     |\n",
    "| Elastic Net                 | {'classifier__C': 0.5, 'classifier__l1_ratio': 1, 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga'} | 0.7062            | 0.6865        | 0.68      | 0.69   | 0.68     | 520     |\n",
    "| KNN                         | {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}                                                          | 0.8920            | 0.8827        | 0.88      | 0.88   | 0.88     | 520     |\n",
    "| Hist Gradient Boosting      | {'classifier__learning_rate': 0.1, 'classifier__max_depth': 9, 'classifier__max_iter': 200}                               | 0.9491            | 0.9538        | 0.95      | 0.95   | 0.95     | 520     |\n",
    "| Gradient Boosting           | Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', GradientBoostingClassifier(max_depth=5, n_estimators=150))])  | 1.0               | 0.9442        | 0.94      | 0.95   | 0.94     | 520     |\n",
    "| Naive Bayes                 | Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', GaussianNB())])                                               | 0.9153            | 0.9404        | 0.94      | 0.94   | 0.94     | 520     |\n",
    "| Ensemble model              | VotingClassifier(estimators=[('svc', SVC(C=128, gamma=0.0078125, probability=True)), ...                                  | 1.0               | 0.95          | 0.95      | 0.95   | 0.95     | 520     |\n",
    "| Ensemble weighted model     | VotingClassifier(estimators=[('svc', SVC(C=128, gamma=0.0078125, probability=True)), ...                                  | 1.0               | 0.9462        | 0.95      | 0.95   | 0.95     | 520     |\n",
    "| Manual ensemble model       | Models: svc, randomForest, knn, HistgradientBoosting, gradientBoosting, naiveBayes                                         | 1.0               | 0.95          | 0.95      | 0.95   | 0.95     | 520     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DatasetZeroes </h1>\n",
    "\n",
    "| Modello                     | Iperparametri                                                                                                                               | Training Accuracy | Test Accuracy | Precision | Recall | F1-score | Support |\n",
    "|-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-------------------|---------------|-----------|--------|----------|---------|\n",
    "| SVC                         | {'classifier__C': 8, 'classifier__gamma': 0.0078125, 'classifier__kernel': 'rbf', 'classifier__probability': True}                        | 0.9278            | 0.9462        | 0.95      | 0.95   | 0.95     | 520     |\n",
    "| Random Forest               | {'classifier__max_depth': 12, 'classifier__n_estimators': 225}                                                                           | 0.9353            | 0.9365        | 0.94      | 0.94   | 0.94     | 520     |\n",
    "| Elastic Net                 | {'classifier__C': 0.5, 'classifier__l1_ratio': 1, 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga'}                   | 0.6296            | 0.6404        | 0.67      | 0.63   | 0.61     | 520     |\n",
    "| KNN                         | {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}                                                                          | 0.6692            | 0.6769        | 0.73      | 0.69   | 0.67     | 520     |\n",
    "| Hist Gradient Boosting      | {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__max_iter': 200}                                             | 0.9510            | 0.9558        | 0.96      | 0.96   | 0.96     | 520     |\n",
    "| Gradient Boosting           | {'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__n_estimators': 150}       | 0.9485            | 0.95          | 0.95      | 0.95   | 0.95     | 520     |\n",
    "| Naive Bayes                 | Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', GaussianNB())])                                                             | 0.9184            | 0.9404        | 0.94      | 0.94   | 0.94     | 520     |\n",
    "| Ensemble model              | VotingClassifier(estimators=[('svc', SVC(C=8, gamma=0.0078125, probability=True)), ...                                                     | 0.9711            | 0.9462        | 0.95      | 0.95   | 0.95     | 520     |\n",
    "| Ensemble weighted model     | VotingClassifier(estimators=[('svc', SVC(C=8, gamma=0.0078125, probability=True)), ...                                                     | 0.9912            | 0.9481        | 0.95      | 0.95   | 0.95     | 520     |\n",
    "| Manual ensemble model       | Models: svc, randomForest, HistgradientBoosting, gradientBoosting, naiveBayes                                                            | 0.9887            | 0.9481        | 0.95      | 0.95   | 0.95     | 520     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DatasetReduced </h1>\n",
    "\n",
    "| Modello                     | Iperparametri                                                                                                                    | Training Accuracy | Test Accuracy | Precision | Recall | F1-score | Support |\n",
    "|-----------------------------|----------------------------------------------------------------------------------------------------------------------------------|-------------------|---------------|-----------|--------|----------|---------|\n",
    "| SVC                         | {'classifier__C': 128, 'classifier__gamma': 0.0078125, 'classifier__kernel': 'rbf', 'classifier__probability': True}          | 0.9784            | 0.9526        | 0.95      | 0.95   | 0.95     | 464     |\n",
    "| Random Forest               | {'classifier__max_depth': 12, 'classifier__n_estimators': 225}                                                                  | 0.9724            | 0.9353        | 0.94      | 0.94   | 0.94     | 464     |\n",
    "| Elastic Net                 | {'classifier__C': 0.5, 'classifier__l1_ratio': 1, 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga'}        | 0.7284            | 0.7672        | 0.76      | 0.73   | 0.74     | 464     |\n",
    "| KNN                         | {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}                                                                 | 0.7970            | 0.7522        | 0.76      | 0.70   | 0.71     | 464     |\n",
    "| Hist Gradient Boosting      | {'classifier__learning_rate': 0.1, 'classifier__max_depth': 9, 'classifier__max_iter': 100}                                    | 0.9784            | 0.9504        | 0.94      | 0.95   | 0.95     | 464     |\n",
    "| Gradient Boosting           | {'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 3, 'classifier__n_estimators': 200} | 0.9754            | 0.9418        | 0.94      | 0.94   | 0.94     | 464     |\n",
    "| Naive Bayes                 | Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', GaussianNB())])                                                    | 0.9239            | 0.9052        | 0.93      | 0.88   | 0.89     | 464     |\n",
    "| Ensemble model              | VotingClassifier(estimators=[('svc', SVC(C=128, gamma=0.0078125, probability=True)), ...                                          | 1.0               | 0.9461        | 0.95      | 0.94   | 0.94     | 464     |\n",
    "| Ensemble weighted model     | VotingClassifier(estimators=[('svc', SVC(C=128, gamma=0.0078125, probability=True)), ...                                          | 1.0               | 0.9483        | 0.95      | 0.95   | 0.95     | 464     |\n",
    "| Manual ensemble model       | Models: svc, randomForest, HistgradientBoosting, gradientBoosting, naiveBayes                                                   | 1.0               | 0.9461        | 0.94      | 0.94   | 0.94     | 464     |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
