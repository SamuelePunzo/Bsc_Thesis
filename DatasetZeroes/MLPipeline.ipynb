{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from combat.pycombat import pycombat\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../Dataset/Merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('MergedDatasetZeroes.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 batches.\n",
      "Adjusting for 0 covariate(s) or covariate level(s).\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors.\n",
      "Finding parametric adjustments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\AppData\\Roaming\\Python\\Python311\\site-packages\\combat\\pycombat.py:159: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.absolute(d_new-d_old)/d_old))  # maximum difference between new and old estimate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting the Data\n"
     ]
    }
   ],
   "source": [
    "sampleID = dataset['SampleID']\n",
    "datasetID = dataset['SampleID'].apply(lambda x: x.split('-')[0]).values\n",
    "indicator = dataset['Label']\n",
    "dataset = dataset.drop(columns=['SampleID', 'Label'])\n",
    "\n",
    "dataset = pycombat(dataset.transpose(), datasetID).transpose()\n",
    "\n",
    "dataset.insert(0, 'SampleID', sampleID)\n",
    "dataset.insert(1, 'Label', indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         SampleID      PatientID  Label  \\\n",
      "0                        0-GSM1026056_600009.0001  0-600009.0001      1   \n",
      "1             0-GSM1026057_600009.0001-FollowUp_1  0-600009.0001      1   \n",
      "2                         0-GSM1026058_41461.0001   0-41461.0001      1   \n",
      "3                         0-GSM1026059_41462.0001   0-41462.0001      1   \n",
      "4                        0-GSM1026060_600029.0001  0-600029.0001      1   \n",
      "...                                           ...            ...    ...   \n",
      "2108  6-GSM2347715_NT142_W18D2-Control-FollowUp_1  6-NT142_W18D2      0   \n",
      "2109  6-GSM2347717_NT041_W18D2-Control-FollowUp_2  6-NT041_W18D2      0   \n",
      "2110  6-GSM2347719_NT142_W18D2-Control-FollowUp_2  6-NT142_W18D2      0   \n",
      "2111  6-GSM2347721_NT041_W18D2-Control-FollowUp_3  6-NT041_W18D2      0   \n",
      "2112  6-GSM2347723_NT142_W18D2-Control-FollowUp_3  6-NT142_W18D2      0   \n",
      "\n",
      "        STEEP1   SEC14L1     YIPF5    SLC1A5        C2      NOL6     SPRR3  \\\n",
      "0     4.370838  8.721593  7.417912  7.425193  5.131296  7.049131  3.583013   \n",
      "1     4.545356  8.751818  7.419999  7.350186  5.147245  7.034854  3.581523   \n",
      "2     4.549717  8.782285  7.426951  7.352916  5.198940  7.033768  3.619811   \n",
      "3     4.767166  8.737405  7.474629  7.334639  5.045285  7.029794  3.545123   \n",
      "4     4.611241  8.765162  7.444116  7.362461  5.224300  7.016216  3.553313   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2108  4.803763  8.694332  7.699366  7.318949  4.975392  6.955651  3.562902   \n",
      "2109  4.697286  8.827418  7.283527  7.368232  5.315038  6.933268  3.588183   \n",
      "2110  3.863508  8.618814  7.729184  7.253268  4.873101  6.926017  3.545890   \n",
      "2111  5.158525  8.826688  7.310339  7.427918  5.443890  6.961344  3.539227   \n",
      "2112  4.747243  8.645304  7.655360  7.273795  4.796590  6.922603  3.651645   \n",
      "\n",
      "      ...     PALLD     KRCC1     RAB25  CATSPERB      PAX8     KCNG1  \\\n",
      "0     ...  5.669328  7.406704  3.832667  3.965903  5.411617  5.559350   \n",
      "1     ...  5.631949  7.419759  3.880221  3.933327  5.427138  5.582391   \n",
      "2     ...  5.681406  7.495984  3.862643  3.922444  5.453633  5.605091   \n",
      "3     ...  5.592508  7.921129  3.791579  4.025124  5.409323  5.494999   \n",
      "4     ...  5.539881  7.659114  3.849954  3.885833  5.413398  5.465897   \n",
      "...   ...       ...       ...       ...       ...       ...       ...   \n",
      "2108  ...  5.782466  7.719678  3.823785  5.525631  5.416486  5.660136   \n",
      "2109  ...  5.690595  7.698274  3.794089  3.345277  5.460764  5.476124   \n",
      "2110  ...  5.638525  7.703289  3.756491  5.500996  5.387297  5.483033   \n",
      "2111  ...  5.721601  7.698232  3.791364  3.459715  5.464287  5.472975   \n",
      "2112  ...  5.718437  7.684801  3.852524  5.283564  5.398482  5.529067   \n",
      "\n",
      "         TRAF3      POLI      OPA1    NKX2-8  \n",
      "0     6.409788  6.143917  8.203767  4.609262  \n",
      "1     6.404660  6.102009  8.146541  4.624083  \n",
      "2     6.400198  6.095752  8.079668  4.620798  \n",
      "3     6.431753  6.181353  8.177918  4.588326  \n",
      "4     6.406614  6.112433  8.187514  4.600809  \n",
      "...        ...       ...       ...       ...  \n",
      "2108  6.384520  6.237530  8.272768  4.585531  \n",
      "2109  6.381342  5.830173  8.253685  4.627394  \n",
      "2110  6.361029  6.195748  8.259073  4.582342  \n",
      "2111  6.398183  6.162931  8.235006  4.604250  \n",
      "2112  6.336473  6.274011  8.188687  4.583651  \n",
      "\n",
      "[2113 rows x 12094 columns]\n"
     ]
    }
   ],
   "source": [
    "def getPatientID(sampleID):\n",
    "    return sampleID.split('-')[0] + '-' + sampleID.split('-')[1].split('_', 1)[1]\n",
    "\n",
    "dataset.insert(1, 'PatientID', dataset['SampleID'].apply(getPatientID))\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "gruppi = dataset.groupby('PatientID')\n",
    "\n",
    "def sanity_check(gruppi):\n",
    "    for group_name, group_data in gruppi:\n",
    "        if 'Control' in group_data['SampleID'].iloc[0]:\n",
    "            for e in group_data['SampleID']:\n",
    "                if not 'Control' in e:\n",
    "                    print(\"Errore in gruppo:\", group_name)\n",
    "                    break\n",
    "        else:\n",
    "            for e in group_data['SampleID']:\n",
    "                if 'Control' in e:\n",
    "                    print(\"Errore in gruppo:\", group_name)\n",
    "                    break\n",
    "\n",
    "sanity_check(gruppi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset di train:\n",
      "(1593, 12094)\n",
      "I malati sono:  695\n",
      "I sani sono:  898\n",
      "\n",
      "Dataset di test:\n",
      "(520, 12094)\n",
      "I malati sono:  245\n",
      "I sani sono:  275\n"
     ]
    }
   ],
   "source": [
    "#n_splits number of re-shuffling & splitting iterations.\n",
    "#test_size represents the proportion of the dataset to include in the test split.\n",
    "#random_state is the seed used by the random number generator.\n",
    "splitter = GroupShuffleSplit(n_splits=2, test_size=0.25, random_state = 42)\n",
    "split = splitter.split(dataset, groups=dataset['PatientID'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train = dataset.iloc[train_inds].sample(frac=1, random_state=42)\n",
    "test = dataset.iloc[test_inds].sample(frac=1, random_state=42)\n",
    "\n",
    "print(\"Dataset di train:\")\n",
    "print(train.shape)\n",
    "print(\"I malati sono: \", sum(train['Label'] == 1))\n",
    "print(\"I sani sono: \", sum(train['Label'] == 0))\n",
    "\n",
    "print(\"\\nDataset di test:\")\n",
    "print(test.shape)\n",
    "print(\"I malati sono: \", sum(test['Label'] == 1))\n",
    "print(\"I sani sono: \", sum(test['Label'] == 0))\n",
    "\n",
    "#malati train 45.12 perc\n",
    "#malati test 42.49 perc\n",
    "#malati sul totale 44.49 perc\n",
    "\n",
    "y_train = train['Label']\n",
    "x_train = train.drop(columns=['SampleID', 'Label', 'PatientID'])\n",
    "\n",
    "y_test = test['Label']\n",
    "x_test = test.drop(columns=['SampleID', 'Label', 'PatientID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, hyperparameters, x_train, y_train, printTrain, njobs): \n",
    "    pipeline = Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', model)])\n",
    "    gridSearch = GridSearchCV(pipeline, param_grid=hyperparameters, cv=5, return_train_score=True, refit=True, n_jobs=njobs, verbose=10, error_score='raise') if printTrain == True else GridSearchCV(pipeline, param_grid=hyperparameters, cv=5, return_train_score=False, refit=True, n_jobs=njobs, verbose=10, error_score='raise')\n",
    "    gridSearch.fit(x_train, y_train)\n",
    "    print(\"Best model:\", gridSearch.best_estimator_, gridSearch.best_params_)\n",
    "    print(\"Best score:\", np.max(gridSearch.cv_results_['mean_test_score']))\n",
    "    print(\"All scores:\", gridSearch.cv_results_['mean_test_score'])\n",
    "    return gridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = trainModel(SVC(), {\n",
    "    'classifier__kernel': ['linear', 'poly', 'rbf'], \n",
    "    'classifier__probability': [True],\n",
    "    'classifier__C': [2**-5, 2**-3, 2**-1, 2**0, 2**1, 2**3, 2**7, 2**9, 2**11, 2**13, 2**15],\n",
    "    'classifier__gamma': [2**-15, 2**-13, 2**-11, 2**-9, 2**-7, 2**-5, 2**-3, 2**-1, 2**1, 2**3, 2**5, 'scale', 'auto']}, x_train, y_train, False, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier', SVC(C=8, gamma=0.0078125, probability=True))]) {'classifier__C': 8, 'classifier__gamma': 0.0078125, 'classifier__kernel': 'rbf', 'classifier__probability': True}\n",
      "Best score: 0.9278011080223182\n",
      "All scores: [0.48842886 0.56371523 0.56371523 0.48842886 0.56371523 0.56371523\n",
      " 0.48842886 0.56371523 0.56371523 0.48842886 0.62397035 0.56371523\n",
      " 0.48842886 0.77840934 0.56371523 0.48842886 0.74450425 0.64468563\n",
      " 0.48842886 0.74450425 0.64280081 0.48842886 0.74450425 0.56371523\n",
      " 0.48842886 0.74450425 0.56371523 0.48842886 0.74450425 0.56371523\n",
      " 0.48842886 0.74450425 0.56371523 0.48842886 0.81294139 0.56371523\n",
      " 0.48842886 0.56371523 0.56371523 0.45385146 0.56371523 0.56371523\n",
      " 0.45385146 0.56371523 0.56371523 0.45385146 0.56371523 0.56371523\n",
      " 0.45385146 0.76209854 0.56371523 0.45385146 0.74889691 0.57689517\n",
      " 0.45385146 0.74450425 0.64280673 0.45385146 0.74450425 0.64468563\n",
      " 0.45385146 0.74450425 0.57688926 0.45385146 0.74450425 0.56371523\n",
      " 0.45385146 0.74450425 0.56371523 0.45385146 0.74450425 0.56371523\n",
      " 0.45385146 0.79472211 0.57438733 0.45385146 0.56371523 0.56371523\n",
      " 0.44443919 0.56371523 0.56371523 0.44443919 0.56371523 0.56371523\n",
      " 0.44443919 0.56623095 0.56371523 0.44443919 0.81733601 0.61330021\n",
      " 0.44443919 0.74450425 0.86818675 0.44443919 0.74450425 0.67732497\n",
      " 0.44443919 0.74450425 0.64468563 0.44443919 0.74450425 0.63840618\n",
      " 0.44443919 0.74450425 0.56371523 0.44443919 0.74450425 0.56371523\n",
      " 0.44443919 0.74450425 0.56371523 0.44443919 0.75956113 0.74512529\n",
      " 0.44443919 0.56371523 0.56371523 0.44129453 0.56371523 0.56371523\n",
      " 0.44129453 0.56371523 0.56371523 0.44129453 0.58129572 0.56810986\n",
      " 0.44129453 0.79786676 0.70871828 0.44129453 0.74450425 0.91650598\n",
      " 0.44129453 0.74450425 0.80788234 0.44129453 0.74450425 0.64594152\n",
      " 0.44129453 0.74450425 0.64657045 0.44129453 0.74450425 0.56371523\n",
      " 0.44129453 0.74450425 0.56371523 0.44129453 0.74450425 0.56371523\n",
      " 0.44129453 0.74826995 0.82925416 0.44129453 0.56371523 0.56371523\n",
      " 0.44945289 0.56371523 0.56371523 0.44945289 0.56371523 0.56371523\n",
      " 0.44945289 0.62397035 0.58505747 0.44945289 0.77840934 0.8091678\n",
      " 0.44945289 0.74450425 0.91964472 0.44945289 0.74450425 0.83612705\n",
      " 0.44945289 0.74450425 0.65096706 0.44945289 0.74450425 0.64970722\n",
      " 0.44945289 0.74450425 0.56371523 0.44945289 0.74450425 0.56371523\n",
      " 0.44945289 0.74450425 0.56371523 0.44945289 0.74450425 0.92090061\n",
      " 0.44945289 0.56371523 0.56371523 0.45259557 0.56371523 0.56371523\n",
      " 0.45259557 0.56371523 0.57878196 0.45259557 0.76209854 0.68297549\n",
      " 0.45259557 0.74889691 0.90708385 0.45259557 0.74450425 0.92780111\n",
      " 0.45259557 0.74450425 0.8436683  0.45259557 0.74450425 0.65159401\n",
      " 0.45259557 0.74450425 0.64970722 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.92152363 0.45259557 0.56371523 0.56811183\n",
      " 0.45259557 0.56371523 0.59134481 0.45259557 0.62397035 0.66041876\n",
      " 0.45259557 0.77840934 0.82736145 0.45259557 0.74450425 0.89452495\n",
      " 0.45259557 0.74450425 0.92027168 0.45259557 0.74450425 0.84868989\n",
      " 0.45259557 0.74450425 0.65159401 0.45259557 0.74450425 0.64970722\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.90646478\n",
      " 0.45259557 0.56811183 0.59763609 0.45259557 0.56371523 0.5229471\n",
      " 0.45259557 0.76209854 0.64908026 0.45259557 0.74889691 0.81482423\n",
      " 0.45259557 0.74450425 0.87947004 0.45259557 0.74450425 0.91901382\n",
      " 0.45259557 0.74450425 0.84868989 0.45259557 0.74450425 0.65159401\n",
      " 0.45259557 0.74450425 0.64970722 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.89892747 0.45259557 0.64343763 0.61644289\n",
      " 0.45259557 0.56623095 0.52667928 0.45259557 0.81733601 0.64281658\n",
      " 0.45259557 0.74450425 0.79662862 0.45259557 0.74450425 0.87821218\n",
      " 0.45259557 0.74450425 0.91901382 0.45259557 0.74450425 0.84868989\n",
      " 0.45259557 0.74450425 0.65159401 0.45259557 0.74450425 0.64970722\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.89892747\n",
      " 0.45259557 0.78030007 0.60387413 0.45259557 0.62397035 0.5172611\n",
      " 0.45259557 0.77840934 0.63968967 0.45259557 0.74450425 0.79474577\n",
      " 0.45259557 0.74450425 0.87821218 0.45259557 0.74450425 0.91901382\n",
      " 0.45259557 0.74450425 0.84868989 0.45259557 0.74450425 0.65159401\n",
      " 0.45259557 0.74450425 0.64970722 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.89892747 0.45259557 0.81043355 0.59698744\n",
      " 0.45259557 0.76209854 0.5153743  0.45259557 0.74889691 0.63843181\n",
      " 0.45259557 0.74450425 0.79474577 0.45259557 0.74450425 0.87821218\n",
      " 0.45259557 0.74450425 0.91901382 0.45259557 0.74450425 0.84868989\n",
      " 0.45259557 0.74450425 0.65159401 0.45259557 0.74450425 0.64970722\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.56371523\n",
      " 0.45259557 0.74450425 0.56371523 0.45259557 0.74450425 0.89892747\n",
      " 0.45259557 0.76835827 0.59950514]\n"
     ]
    }
   ],
   "source": [
    "svc = joblib.load('../../Modelli/DatasetZeroes/svc.pkl')\n",
    "print(\"Best model:\", svc.best_estimator_, svc.best_params_)\n",
    "print(\"Best score:\", np.max(svc.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", svc.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = trainModel(RandomForestClassifier(), {\n",
    "    'classifier__n_estimators': [i for i in range(150, 251, 25)],\n",
    "    'classifier__max_depth': [i for i in range(4, 13)]}, x_train, y_train, True, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 RandomForestClassifier(max_depth=12, n_estimators=225))]) {'classifier__max_depth': 12, 'classifier__n_estimators': 225}\n",
      "Best score: 0.935338419983833\n",
      "All scores: [0.92340845 0.92215059 0.9246604  0.92026774 0.92215059 0.92215059\n",
      " 0.92277755 0.92277952 0.92152166 0.92026774 0.92403147 0.92277755\n",
      " 0.92340648 0.92591629 0.92277755 0.92341239 0.92654325 0.92780111\n",
      " 0.92654127 0.9246604  0.9284261  0.92905305 0.92716823 0.92968396\n",
      " 0.92403344 0.92968001 0.92842807 0.92717415 0.92779716 0.93156878\n",
      " 0.93031486 0.92968987 0.93156878 0.92843595 0.93407662 0.92843398\n",
      " 0.93031092 0.93219771 0.93408253 0.93345557 0.93407464 0.929057\n",
      " 0.93094773 0.93533842 0.93156878]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94       275\n",
      "           1       0.90      0.98      0.94       245\n",
      "\n",
      "    accuracy                           0.94       520\n",
      "   macro avg       0.94      0.94      0.94       520\n",
      "weighted avg       0.94      0.94      0.94       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest = joblib.load('../../Modelli/DatasetZeroes/randomForest.pkl')\n",
    "print(\"Best model:\", randomForest.best_estimator_, randomForest.best_params_)\n",
    "print(\"Best score:\", np.max(randomForest.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", randomForest.cv_results_['mean_test_score'])\n",
    "\n",
    "y_pred = randomForest.predict(x_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticNet = trainModel(LogisticRegression(), {\n",
    "    'classifier__C': [2**-7, 2**-5, 2**-3, 2**-1, 2**0, 2**1, 2**3, 2**7, 2**9, 2**11],\n",
    "    'classifier__penalty': ['elasticnet'],\n",
    "    'classifier__l1_ratio': [0, 0.0001, 0.1, 0.25, 0.5, 0.75, 1],\n",
    "    'classifier__solver': ['saga']}, x_train, y_train, True, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.5, l1_ratio=1, penalty='elasticnet',\n",
      "                                    solver='saga'))]) {'classifier__C': 0.5, 'classifier__l1_ratio': 1, 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga'}\n",
      "Best score: 0.6296267818063525\n",
      "All scores: [0.51917943 0.52106031 0.56371523 0.56371523 0.56371523 0.56371523\n",
      " 0.56371523 0.47145167 0.47019578 0.57438733 0.56371523 0.56371523\n",
      " 0.56371523 0.56371523 0.45763096 0.45825595 0.54865243 0.6233493\n",
      " 0.60137813 0.56497309 0.56371523 0.45888094 0.46013683 0.47395359\n",
      " 0.51664005 0.56624081 0.60703259 0.62962678 0.45700006 0.45888094\n",
      " 0.46578932 0.48149287 0.51789594 0.54677353 0.57000848 0.45825595\n",
      " 0.45825595 0.4639045  0.46641825 0.48211983 0.49593068 0.51789594\n",
      " 0.45637507 0.45825398 0.45888291 0.46076379 0.46453146 0.46767217\n",
      " 0.46704521 0.45825595 0.45825792 0.45888291 0.45762702 0.45700203\n",
      " 0.46139272 0.45888291 0.45700203 0.45888291 0.45888291 0.45825792\n",
      " 0.45888291 0.45888488 0.45825595 0.45762899 0.45825989 0.45950987\n",
      " 0.45951184 0.45888291 0.46076576 0.457004  ]\n"
     ]
    }
   ],
   "source": [
    "elasticNet = joblib.load('../../Modelli/DatasetZeroes/elasticNet.pkl')\n",
    "print(\"Best model:\", elasticNet.best_estimator_, elasticNet.best_params_)\n",
    "print(\"Best score:\", np.max(elasticNet.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", elasticNet.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = trainModel(KNeighborsClassifier(), {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'classifier__weights': ['uniform', 'distance'] }, x_train, y_train, True, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier', KNeighborsClassifier(n_neighbors=3))]) {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}\n",
      "Best score: 0.6691863330770292\n",
      "All scores: [0.66918633 0.66918633 0.63715818 0.63715818 0.60953648 0.60953648\n",
      " 0.57564125 0.57564125 0.55995347 0.55995347 0.54235918 0.54298811\n",
      " 0.52666351 0.52666351]\n"
     ]
    }
   ],
   "source": [
    "knn = joblib.load('../../Modelli/DatasetZeroes/knn.pkl')\n",
    "print(\"Best model:\", knn.best_estimator_, knn.best_params_)\n",
    "print(\"Best score:\", np.max(knn.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", knn.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistGradientBoosting = trainModel(HistGradientBoostingClassifier(), {\n",
    "    'classifier__max_depth': [3, 5, 7, 9, 11],\n",
    "    'classifier__max_iter': [100, 150, 200, 250],\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001]}, x_train, y_train, False, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 HistGradientBoostingClassifier(max_depth=5, max_iter=200))]) {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__max_iter': 200}\n",
      "Best score: 0.9510301453047061\n",
      "All scores: [0.93722324 0.94161787 0.94789535 0.9491473  0.94977426 0.94977031\n",
      " 0.95103015 0.95040121 0.94474872 0.94788746 0.94788943 0.94851639\n",
      " 0.94600264 0.94725853 0.94851639 0.94789141 0.9472605  0.9466296\n",
      " 0.94788943 0.9491473  0.92340648 0.92152954 0.92654916 0.9296879\n",
      " 0.9328365  0.93973502 0.94099288 0.94099485 0.93033063 0.93221348\n",
      " 0.93911003 0.93911003 0.93158455 0.93158455 0.93785611 0.93848307\n",
      " 0.93033063 0.93158455 0.93722718 0.93722718 0.56371523 0.87761085\n",
      " 0.8926559  0.89453875 0.56371523 0.8637862  0.88448769 0.89389996\n",
      " 0.56371523 0.86941898 0.8882593  0.89642357 0.56371523 0.87004594\n",
      " 0.88763037 0.89704856 0.56371523 0.86941898 0.88763037 0.89704856]\n"
     ]
    }
   ],
   "source": [
    "HistGradientBoosting = joblib.load('../../Modelli/DatasetZeroes/HistGradientBoosting.pkl')\n",
    "print(\"Best model:\", HistGradientBoosting.best_estimator_, HistGradientBoosting.best_params_)\n",
    "print(\"Best score:\", np.max(HistGradientBoosting.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", HistGradientBoosting.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoosting = trainModel(GradientBoostingClassifier(), {\n",
    "    'classifier__max_depth': [3, 5, 7, 9, 11],\n",
    "    'classifier__max_iter': [100, 150, 200, 250],\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001]}, x_train, y_train, False, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Pipeline(steps=[('Scaling', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 GradientBoostingClassifier(max_depth=5, n_estimators=150))]) {'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__n_estimators': 150}\n",
      "Best score: 0.9485242798840717\n",
      "All scores: [0.93595552 0.93784231 0.94160801 0.94162576 0.94852428 0.94727233\n",
      " 0.9303188  0.93597721 0.93596144 0.92466434 0.91963684 0.92152363\n",
      " 0.93408253 0.93220362 0.93471343 0.92026774 0.92654719 0.93031683\n",
      " 0.56371523 0.88011672 0.89892549 0.56371523 0.87760494 0.90834566\n",
      " 0.56371523 0.90521875 0.90772658]\n"
     ]
    }
   ],
   "source": [
    "GradientBoosting = joblib.load('../../Modelli/DatasetZeroes/GradientBoosting.pkl')\n",
    "print(\"Best model:\", GradientBoosting.best_estimator_, GradientBoosting.best_params_)\n",
    "print(\"Best score:\", np.max(GradientBoosting.cv_results_['mean_test_score']))\n",
    "print(\"All scores:\", GradientBoosting.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayes = Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', GaussianNB())])\n",
    "print(naiveBayes.fit(x_train, y_train).score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9183929692404269\n"
     ]
    }
   ],
   "source": [
    "naiveBayes = joblib.load('../../Modelli/DatasetZeroes/naiveBayes.pkl')\n",
    "print(naiveBayes.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance dei vari modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC:\n",
      "Iperparametri:  {'classifier__C': 8, 'classifier__gamma': 0.0078125, 'classifier__kernel': 'rbf', 'classifier__probability': True}\n",
      "Training accuracy:  0.9278011080223182\n",
      "Test accuracy:  0.9461538461538461 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95       275\n",
      "           1       0.92      0.97      0.94       245\n",
      "\n",
      "    accuracy                           0.95       520\n",
      "   macro avg       0.95      0.95      0.95       520\n",
      "weighted avg       0.95      0.95      0.95       520\n",
      " \n",
      "\n",
      "\n",
      "Random Forest:\n",
      "Iperparametri:  {'classifier__max_depth': 12, 'classifier__n_estimators': 225}\n",
      "Training accuracy:  0.935338419983833\n",
      "Test accuracy:  0.9365384615384615 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94       275\n",
      "           1       0.90      0.98      0.94       245\n",
      "\n",
      "    accuracy                           0.94       520\n",
      "   macro avg       0.94      0.94      0.94       520\n",
      "weighted avg       0.94      0.94      0.94       520\n",
      " \n",
      "\n",
      "\n",
      "Elastic Net:\n",
      "Iperparametri:  {'classifier__C': 0.5, 'classifier__l1_ratio': 1, 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga'}\n",
      "Training accuracy:  0.6296267818063525\n",
      "Test accuracy:  0.6403846153846153 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.87      0.72       275\n",
      "           1       0.72      0.38      0.50       245\n",
      "\n",
      "    accuracy                           0.64       520\n",
      "   macro avg       0.67      0.63      0.61       520\n",
      "weighted avg       0.66      0.64      0.62       520\n",
      " \n",
      "\n",
      "\n",
      "KNN:\n",
      "Iperparametri:  {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}\n",
      "Training accuracy:  0.6691863330770292\n",
      "Test accuracy:  0.676923076923077 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.47      0.61       275\n",
      "           1       0.60      0.91      0.73       245\n",
      "\n",
      "    accuracy                           0.68       520\n",
      "   macro avg       0.73      0.69      0.67       520\n",
      "weighted avg       0.73      0.68      0.66       520\n",
      " \n",
      "\n",
      "\n",
      "Hist Gradient Boosting:\n",
      "Iperparametri:  {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__max_iter': 200}\n",
      "Training accuracy:  0.9510301453047061\n",
      "Test accuracy:  0.9557692307692308 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96       275\n",
      "           1       0.93      0.98      0.95       245\n",
      "\n",
      "    accuracy                           0.96       520\n",
      "   macro avg       0.96      0.96      0.96       520\n",
      "weighted avg       0.96      0.96      0.96       520\n",
      " \n",
      "\n",
      "\n",
      "Gradient Boosting:\n",
      "Iperparametri:  {'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__max_depth': 5, 'classifier__n_estimators': 150}\n",
      "Training accuracy:  0.9485242798840717\n",
      "Test accuracy:  0.95 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       275\n",
      "           1       0.93      0.97      0.95       245\n",
      "\n",
      "    accuracy                           0.95       520\n",
      "   macro avg       0.95      0.95      0.95       520\n",
      "weighted avg       0.95      0.95      0.95       520\n",
      " \n",
      "\n",
      "\n",
      "Naive Bayes:\n",
      "Iperparametri:  Pipeline(steps=[('Scaling', MinMaxScaler()), ('classifier', GaussianNB())])\n",
      "Training accuracy:  0.9183929692404269\n",
      "Test accuracy:  0.9403846153846154 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.89      0.94       275\n",
      "           1       0.89      0.99      0.94       245\n",
      "\n",
      "    accuracy                           0.94       520\n",
      "   macro avg       0.94      0.94      0.94       520\n",
      "weighted avg       0.95      0.94      0.94       520\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prettyPrint(model, name):\n",
    "    if isinstance(model, Pipeline):\n",
    "        print(name + \":\")\n",
    "        print(\"Iperparametri: \", model)\n",
    "        print(\"Training accuracy: \", model.score(x_train, y_train))\n",
    "        print(\"Test accuracy: \", model.score(x_test, y_test), \"\\n\")\n",
    "        print(classification_report(y_test, model.predict(x_test)), \"\\n\\n\")\n",
    "    else:\n",
    "        print(name + \":\")\n",
    "        print(\"Iperparametri: \", model.best_params_)\n",
    "        print(\"Training accuracy: \", model.best_score_)\n",
    "        print(\"Test accuracy: \", model.score(x_test, y_test), \"\\n\")\n",
    "        print(classification_report(y_test, model.predict(x_test)), \"\\n\\n\")\n",
    "\n",
    "for model, name in [(svc, \"SVC\"), (randomForest, \"Random Forest\"), (elasticNet, \"Elastic Net\"), (knn, \"KNN\"), (HistGradientBoosting, \"Hist Gradient Boosting\"), (GradientBoosting, \"Gradient Boosting\"), (naiveBayes, \"Naive Bayes\")]:\n",
    "    prettyPrint(model, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
